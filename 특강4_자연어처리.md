# 자연어처리 기술 소개 및 응용 사례 - 임수종
 
https://wikidocs.net/book/2155   
https://book.naver.com/bookdb/book_detail.nhn?bid=14797086  
자연어 프로젝트: 메일 

----
#### 전문가 시스템 - rule base system  
- rule 범위 내에서는 완벽히 작동  
- rule 외부의 문제는 답하지 못하지 못하기 때문에 2차 암흑기 옴  

#### 자연어 처리 개념
자연어 처리 <-> 인공어(문법에 맞춰 쓰고 문법 틀리면 오류 나는)  
1. 데이터가 숫자가 아니라 자연어  
2. 연구 대상이 언어이기 때문에 경계가 모호  
    자연어 처리하려면 문과 사람들과 공동 작업 해야함  
3. 자연어는 규칙으로 대체 하지 못함
    rule이 바뀌면 개발자가 수정하는 것이 아닌 데이터를 받아 학습해서 자동 업데이트  
4. 다양한 응용   

### 자연어 심층이해 기술  

```
1. 단어 이해 
  - 형태소 분석, 어휘분석, 개체명 인식
2. 문장 이해
  - 구문 분석  
3. 문맥 이해
  - 상호참조해결  
```

```
전처리 -> 자연어 구문 이해(문법 이해) -> 자연어 의미 이해
```

```
형태소 분석 -> 개체명 인식 -> 어휘 의미 분석 -> 의존 구문 분석 -> 의미역 인식 -> 상호참조해결 ->무형대용어 생략 복원
```

#### 형태소 분석
어절을 나누는 것도 어렵고 태깅 하는 것도 어려움  


#### 형태소 태그셋  
세종 태그셋  


#### Lattice 기반 형태소 분석[Mecab: EMNLP 2004]  
회사에서 많이 씀!!  

#### 개체명 인식  
주로 고유 명사 인식, 단어에 대한 의미 인식  

#### 기존 언어 분석 : 오류 전파 방지
```
입력 문서 ->  형태소 분석 -> 어휘의미 중의성 해소 -> 개체명 인식 -> 구문 분석 -> 의미역 인식 -> 상호참조해결 -> 무형대용어 복원 -> 분석 결과  
순차적 언어분석 방법은 앞 단계에서 오류가 뒷 단계로 전파됨  
```
<br>
----
## ETRI AI Open 사용한 실습
개발 기이드 -> 언어 분석 -> API

#### colab

```
#-*- coding:utf-8 -*-
import urllib3
import json
 
# 언어 분석 기술 문어/구어 중 한가지만 선택해 사용
# 언어 분석 기술(문어)
openApiURL = "http://aiopen.etri.re.kr:8000/WiseNLU" 
# 언어 분석 기술(구어)
#openApiURL = "http://aiopen.etri.re.kr:8000/WiseNLU_spoken"
 
accessKey = "04933751-7169-4df1-beba-b9b89c9a61d0"
analysisCode = "srl"
text = "법무부는 6일 오후 3시 정부과천청사에서 검찰인사위원회를 열고 검사장급 이상 고위 검찰 간부 승진·전보 인사를 논의한다. 추미애 법무부 장관 취임 이후 두 번째로 열리는 이번 인사 결과는 이날 오후 늦게나 내일 오전쯤 나올 예정이다."
 
#// 언어 분석 기술(문어)

text += """윤동주(尹東柱, 1917년 12월 30일 ~ 1945년 2월 16일)는 한국의 독립운동가, 시인, 작가이다. 
        중국 만저우 지방 지린 성 연변 용정에서 출생하여 명동학교에서 수학하였고, 숭실중학교와 연희전문학교를 졸업하였다. 숭실중학교 때 처음 시를 발표하였고, 1939년 연희전문 2학년 재학 중 소년(少年) 지에 시를 발표하며 정식으로 문단에 데뷔했다.
        일본 유학 후 도시샤 대학 재학 중 , 1943년 항일운동을 했다는 혐의로 일본 경찰에 체포되어 후쿠오카 형무소(福岡刑務所)에 투옥, 100여 편의 시를 남기고 27세의 나이에 옥중에서 요절하였다. 사인이 일본의 생체실험이라는 견해가 있고 그의 사후 일본군에 의한 마루타, 생체실험설이 제기되었으나 불확실하다. 사후에 그의 시집 《하늘과 바람과 별과 시》가 출간되었다.
        일제 강점기 후반의 양심적 지식인으로 인정받았으며, 그의 시는 일제와 조선총독부에 대한 비판과 자아성찰 등을 소재로 하였다. 그의 친구이자 사촌인 송몽규 역시 독립운동에 가담하려다가 체포되어 일제의 생체 실험으로 의문의 죽음을 맞는다. 1990년대 후반 이후 그의 창씨개명 '히라누마'가 알려져 논란이 일기도 했다. 본명 외에 윤동주(尹童柱), 윤주(尹柱)라는 필명도 사용하였다."""
#// 언어 분석 기술(구어)
#text += "네 안녕하세요 홍길동 교숩니다"+;
 
requestJson = {
    "access_key": accessKey,
    "argument": {
        "text": text,
        "analysis_code": analysisCode
    }
}
 
http = urllib3.PoolManager()
response = http.request(
    "POST",
    openApiURL,
    headers={"Content-Type": "application/json; charset=UTF-8"},
    body=json.dumps(requestJson)
)
 
print("[responseCode] " + str(response.status))
print("[responBody]")
print(str(response.data,"utf-8"))
```

#### https://jsonformatter.curiousconcept.com/
실행 결과 넣어서 편하게 보깅  
> morp: 분석결과  
    > lemma : 단어 자체, 단어에 대한 뜻  
    > type : 품사 타입  
    > 형태소 분석 기본적으로 나눌 수 있는 단위까지 나눠놓고 합쳐놓을 일 있을 때 합침  
    > 그러나 개발 특성에 따라 합친 형태로 다뤄도 됨  
    
> WSD

> NE : 개체명에 해당 되는 것만 분석된 결과 나옴  

<br>    
```
뉴스에서 명사(NNG), 동사(VV) 추출하기
```
----

## 자연어 처리 응용(QA)
자연어 질의 응답 기술 : 자연어로 기술된 문제의 의미를 이해하고, 정답을 추론하여 생성함  

#### 초기 QA 기술
오답 문장 - 1. 상호참조해결이 필요한 경우 2. 생략 성분 복원이 필요한 경우


#### 딥러닝 적용 자연어
초기에는 복잡한 경로를 빨리 풀어냈고 이것이 언어에도  적용될 줄 몰랐음  
데이터가 다르기 때문에 같은 알고리즘을 써도 서로 다른 학습 모델이 생김  

#### MRC의 전제
문서 안에 답이 있다  
질문하면 답이 해당하는 부분을 숫자로 알려줌  

딥러닝을 썼을 때 input이 질문, 정답이 들어있는 문서 / output: 정답이 들어있는 문서의 어절 위치(앞 번호 1개, 끝 번호 1개)  
엑소브레인에서 정답 찾아내는 것과 전혀 다른 방식!!  

//HCNS 챌린지???

#### MRC 시스템 예(BiDAF)
Query2Context Context2Query 2개를 만듦

전에 잘 안풀린 문제를 안써본 기법을 적용해서 풀 수 있는 능력을 가져야 함  
모든 것을 벡터화 시키기 때문에 이미지든 text든 벡터로 오기 때문에 상관 없음  

#### 문서 분류
각 분류 당 200문서는 있어야 문서의 특정 catch 할 수 있음  
분류체계가 명확해야 좋음(사람이 보기에 명확하지 않으면 학습 데이터가 명확하지 않을 것이고그럼 그거으로 학습한 인공지능은 성능이 떨어질 것!)  

- 기계 학습 : 특징 추출  
 
#### 한국어 단위 적용
무조건 2글자씩 끊어서(띄어쓰기상관하지 않고)  

ngram

#### SVM 기반 자동 분류
#### 딥러닝(CNN) 기반 자동 분류  
이미지를 부분적으로 자르면 겹치는 부분이 있기 때문에 1차원으로 하는 것보다 특성이 살아있음  

#### 분류 성능 향상
softmax를 써서 top3 보여주고 고르게 해서 정확도 높힘   
어차피 골라야하니 top3로 보여주고 있으면 고르고(정확도 80%정도) 없으면 원래대로 골라라  

1 Level에만 해당하는 클래스로 학습을 함 -> top3 보여줘서 정확도 높힘  


### 감성분석  
자동 분류 중 하나 - 클래스가 긍정,부정,중립 3개인!!

#### 가짜 뉴스 판별  
접근방법: 제목의 벡터의 위치와 단락 내요의 벡터 위치 계산(유사도 계산), 각 단락(혹은 문장)의 벡터 위치 계산(유사도계산)  
공통사항 : RNN, CNN 사용  
입상/비입상 차이점 : **앙상블 기법 활용***, 딥러닝 사용시 OOV 문제 대응, 축약어 등을 위해 자모단위 접근, 유의어/반의어 관계 활용, 복잡한 언어분석 사용(구문관계, 의미역 인식 등), Phrase/sentence embedding => 방법론 다양하게 사용하고, 이미지면 자르거나 회전해서 데이터 많은 것처럼 불리기    



#### DARPA  

#### Federated Learning
데이터가 한 곳에 모여 학습시키는 것은 컴퓨팅 파워가 되는 큰 기업만 가능하므로 불공평! 그래서 Federated Learning 개념 나옴  
Weight vector를 자기가 업데이트 하고 다시 서버로 보냄, 각각의 데이터에 최적화된 weight vector를 이용해 중앙에서 최적의 weight vector 업데이트  
ex. 의료데이터  
<br>
---
### 자연어처리 응용을위한 준비 - 전처리 및 언어모델 
#### 토큰화  
- 문장 분리 학습
 - binary 분류를 이용하기도 하고, 완전하지는 않음  
 - 한국어 토큰화 어렵 : 교착어, 띄어쓰기 미준수 등  
 - 한국어 토큰화 : 주로 형태소 분석 후 이 결과를 이용하는 방법  
 
#### 토큰화 실습 
문장분리  - kss  

#### 정수 인코딩
- Vocab 내의 단어에 고유 번호를 부여  
- OOV
- 정수 인코딩 후 문장을 벡터 형태로 표현함  

#### one-hot-encoding
단어 자체를 정수가 아닌 벡터로 다루고 싶을 때
1단계: 고유 숫자 부여
2단계: 해당하는 벡터 위치만 1을 부여하고 나머지는 0을 부여함  

문제점: Vocab 크기에 따라 비효율의 문제, 딥러닝 분류의 경우 적은 수의 분류 집합에 대해 적용  

#### 언어표현 : 분포 가설
벡터로 표현해서 거리 측정으로 Similar meaning 판단  

임베딩: 자주 같이 쓰는 단어 벡터 위치 비슷하게!  

단어에 숫자 부여하고 유사한 벡터 부여한다! 기준은? 사람이 자주 같이 쓰는거  

의미 휴사도 계산: 단어의 공간상 위치 결정? - Word2vec, FastText, Glove...  

#### 임베딩 통계 정보
- 빈도     
  - 순서를 고려하지 않음   
  - TF-IDF : TF, DF, IDF, N   
      - TF-IDF는 적은 문서에서 쓰인 단어가 값 큼  
      - 총 빈도가 같아도 IDF 값 높은게 변별력 갖는 단어임!!   
      - 문서의 핵심어 추출, 검색 결과 순위 결정 => 똑같은 단어가 1번문서, 2번 문서에 나오면 검색했을 때 문서 2개가 나올텐데 어떤 문서가 먼저 나올지!!   
                                                TF-IDF 값이 높은 애를 먼저 보여줌    
  
  - 문서를 대표하는 단어: TF-IDF 큰 단어  
  - 가장 유사한 문서: TF-IDF 값이 겹치는 단어가 많은 문서들  


- 순서
 언어 모델: ELMO, GPT  
 
 #### 통계적 언어 모델: 앞의 n개 단어를 기반으로 다음 단어를 예측  
  ex. 음성인식: 자주 쓰는 단어를 근거로 다음에 올 말 예측!  
  문제점: oov 단어는 다른 단어와의 유용성과 상관없이 0이 됨!!  
  
 #### Word2vec : CBOW
  맥락으로 부터 타깃 추측  
  활성화 함수 없으면 함수 나올 때까지!!  
   
  단어 2개를 주고 그 사이에 있는 단어를 맞히는 과정을 반복하면서 Weight가 좋아짐!! -> 학습 과정  
  weight vector가 자기 위치로 잘 지정되도록!!  
  
 #### Word2vec : Skip-gram
 나를 기준으로 앞 뒤 단어 맞히기 학습  
 low: weight vector , colunms: 디멘져?
 Cbow보다 성능 좋다  
 
  #### Word2vec 문제점  
  한국어로 가져와서 학습 시켰을 때 보름 걸림!! -> Nagative Sampling 안해서!!  
  마지막에 원핫으로해서 소프트맥스(원핫의 element를 기준으로 합이 1이 되게 만드는 특성 갖고있음)  
  50만개를 5만번 update 하기 때문에 시간이 많이 걸림!!!
  -> Vovabulary size가 크면 대부분 의미없는 연산  
      => 해결: 타겟팅해서 Nagative Sampling  
      
  #### Word2vec 한계점
  - 단어의 형태학적 특성을 반영하지 못함  
     ex. 연구소, 연구, 연구원 등을 형태적, 의미적으로 유사한데 개별적으로 임베딩하므로 이러한 특성 반영 X
  - 희소한 단어를 임베딩하기 어려움  
    
  - out-of-vocabulary를 처리할 수 없음  
  
  #### Word2vec 해결 시도
  단어 수준 임베딩 : FastText  
    각 단어를 문자 단위 n-gram으로 표현!! -> 형태학적 특성을 반영하기 위한 방법   
    작은 단위를 사용하여 미등록 단어 추정 가능 -> oov 문제 해결   
  
  #### 단어 수준 임베딩 :Glove
  학습의 목적 : 워드 벡터의 로그값이 ~~~   
  각 벡터의 내적 값이 그 값이 나오도록!!!!!    
   
   
  #### 문장 수준 임베딩 : Doc2Vec
  문서의 id도 하나의 학습 요소가 되는 것!! Word2Vec + 문장 정보  
  => 문장이 마치 단어처럼 특정 뜻을 가진다고 가정하고 문장 정보를 추가하여 학습 시킴!!  
  => 좀 더 큰 맥락을 볼 수 있게 됨  
  
  
  
