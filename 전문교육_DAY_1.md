# AI에 대한 보안 공격과 방어

Safety : 공격자 없는데 시스템 결함 발견  
Security: 공격자 있다고 가정  

#### Evasion Attack : 데이터를 변조해서 다른 데이터로 인식하도록 하는 공격   
- 사람이 보기에는 똑같은데 다르게 인식하도록 만드는 것   
- 원리: Target classifier를 속일 수 있도록 노이즈를 더함   
    - 변조율 최소(사람에 의한 탐지를 방지) & 최대 오인식을 하는 노이즈 (a) 찾기  
    
#### 환경 설치
https://www.dropbox.com/s/jfloz080tg8flny/aisecv.ova?dl=0    

구동 - id: aisec, pw : 1234  

#### Muti Target  
요즘 딥러닝은 object function만 잘 설정하면 됨  

#### Resticted Evasion  
제한된 영역만 변조하며 evasion 공격  
제한이 가해지면 100% 성공하지는 못함   

#### GAN : 이미지 뿐만 아니라 데이터를 변환해서 생성 가능   
Classifier에게 보여주고 나서 진짜인지 아닌지 feedback 받아 더 잘 생성할 수 있도록 한 것이 GAN  
네트워크 침입 패턴도 GAN을 가지고 하는 경우 O  
데이터 부족한 경우 GAN 이용해서 데이터 추가적으로 생성하는데도 사용할 수 있음    

#### 실습  
```
$ source adv_venv/bin/activate  
```

#### 텐서플로우 
playground.tensorflow.org    

#### Transferablity
- 공격 환경(조건)  
    - 내 모델과 타켓 모델이 비슷할수록 공격 성공률 향상  
    - 타겟 모델 정보를 많이 알 수록 유리  

### Suvstitute model  
화이트 박스 공격로 공격 성공하면 블랙 박스 모델로 공격!!   
- 블랙 박스 환경에서 쿼리를 해서 레이블을 만듦  

- 내가 준비하는 이미지 데이터에 따라 선응이 달라짐  
- 방어 관점에서는 쿼리 개수를 제한하면 됨  

#### Gradient Estimation  
- 블랙 박스 환경에서는 얻을 수 없기 때문에 화이트 박스 사용   
- confidence 바탕으로 gradient 추정  
- X-h와 X+h 사이의 값을 가지고 gradient 값을 추정하여 gradient 값 사용!    

- Estimator가 있어서 개체 모델이 없어도 마치 블랙 박스 모델이 화이트 박스 모델인 것처럼 사용!!  
- confidence가 반드시 필요!  
- confidence를 활용해서 공격을 하니 confidence를 주지 말아야 한다는 의견도 있으나 대부분 줌   

#### 모델 앙상블 
각각 다른 모델에 대해서 공격 성공하는 모델을 만들자  
그걸 가지고 타겟 모델을 공격하면 Transferablity가 높아져서 공격 확률이 높아질 것이다!!  

1) 공격하려고 2) 변조를 많이 하지 않기 위해  
=> 여러개 모델들의 경계를 넘어가려면 변조 많이해야함!!   

#### Retrainung substitute model
최대한 작은 쿼리로 높은 성공률 보이는 모델 만드는 것이 아이디어  
정한 클래스에 대해서만 공격만들 수 있으므로 훨씬 쿼리가 줄어듦  

### Mitigation    
#### 학습단계 문제 및 보호방안
1. 학습 데이터 오렴  
    - 데이터 출처 관리  
    - 오염 데이터 유입 방지  
    - 오염 데이터 탐지/필터링  
    - AI 시스템 정보 유출 방지    

2. 데이터 / 개인정보 유출  
    - 데이터 접근제어 및 보호  
    - 데이터 비식별 조치    

3. 모델 취약성 검사  
    - 모의 공격 테스트  
    - 모델 평가  
    
#### 활용단계 모델 및 보호방안  
1. 실시간 학습 데이터 오염  
2. 회피 공격  
3. 모델 탈취  
4. 데이터 추출 공격  
5. 모델 추출 공격  
6. 유사 모델 활용 공격  

#### 탐지 + 필터링
왜곡이 많을수록 탐지가 잘 됨  

왜곡을 적게 하면 필터링에 걸리고 왜곡을 크게하면 탐지에 걸림  

#### 적대적 예제 탐지 + 필터링
필터링을 먼저하고 필터링한 결과와 그냥 결과를 모델에 넣어 공격인지 여부 판단  

#### LIME 
어떤 영역이 중요한 영역인지 찾기 위해 이미지 부분 부분을 가려보는 것  
- 이미지에서 일부 영역을 회색으로 가려서 모델의 출력값 변화 관찰  
- 출력값의 변화가 작으면 : 중요하지 않은 영역      
- 출력값의 변화가 크면 : 중요한 영역  

정상 샘플 : 주요 영역이 주로 객체에 집중되어 있음  
적대적 예제 LIM : 주요 영역이 주로 분산되어 있음  

주요 영역만 target model에 넣음 -> output이 다르면 정상 sample 에서는 객체 영역이 입력되니 전체 이미지가 입력되는 것과 비슷한 분류 결과가 나올 것임  

### 학습 데이터 프라이버시
데이터를 고객으로부터 수집하거나 생성된 데이터를 머신러닝에 사용하면 안됨    
우리나라법은 누구인지 식별이 안되면 개인벙보보호법에 저촉되지 않음 -> 그러나 프라시버시 침해일 수도 있음  

#### 실습
```
source adv_venc/bin/activate
pip list
cd adv
ls
cd 01_FGSN
ls
vi attack.py
python attack.py
history //여태까지 한 명령어 나옴
```

```
visualize 주석풀기
```
```
def save

for 부분
png_name
save_png
imsave?
그리고 실행
```

#### 실습 2








