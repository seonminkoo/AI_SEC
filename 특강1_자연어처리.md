# 특강1_자연어처리
## 딥러닝을 위한 수학, 딥러닝 학습 원리 이해

- 지도학습  
- 비지도 학습: 군집화 자동화!  
- 강화 학습   
    알파고 은퇴:   
        1. 알려진 알고리즘 다 사용해봤음   
        2. 비즈니스적 이득 다 취함   
        3. 마지막으로 한 게 강화학습! -> 알파고끼리 바둑을 두게 함 -> 데이터가 결과가 빨리 나올 것 -> 이를 통해 배움  
         
#### 분류 모델:  
  #### 지도 학습 전에 비지도 학습 쓰는 법!  
    수집된 이미지를 가지고 비지도 학습 알고리즘으로 10 덩어리로 나눠!   
    10덩어리가 이상하게 분류된 것 눈으로 보고 분류    
    다시 10덩어리 가지고 분류  
  
#### - 회귀 모델과 분류 모델 차이점  
  회귀 모델: 나누는 선이 자체가 답  
  분류 모델: 선을 보고 위치에 따라 x, y 처럼 클래스를 나눌 수 있는 것!   
  
#### 회귀 모델
  곡선이 생기는 이유 오차를 뒀기 때문!  
  예측 함수는 다항식으로 함  
  학습: wo, w1, w2....wm 를 보고 값과 비슷하게 나올 수 있는 다음 값을 찾는 것, wm에 대한 것을 학습하는 것!   
        m의 값에 따라 sin(2ㅠx)와 가장 비슷하게 나오는 거!  
        **=> 정답을 알고 있는 ㅎ학습 데이터를 이용하여 오차 범위 내에서 예측 값이 되도록 하는 과정**      
                오차 범위 내: overfiting, underfiting  
                      오차의 기준이 내가 가지고 있는 데이터 안에서만 말하는 것임  
                      오차가 0이 된다고 좋은게 아님!!  
  
  학습의 척도 -> 손실 함수(내가 예측한 값과 얼마나 오차가 있는지)    
    손실값이(선형 직선)이 계속 바뀌는데 오차가 목적한 범위 내에 들어왔을 때 끝남!   
    학습이 안되는 경우: 손실값이 줄지 않고 변함없을 때(손실함수가 잘못됐거나 알고리즘이 잘못됐거나 등등)  
    
### 손실함수  
1. n이 들어간 이유: 평균이라서!, 학습을 할 때 관여된 입력 데이터 개수, 1개 당 손실 계산 가능   


#### 경사 하강법  
'딥러닝을 위한 수학' 예제코드 1번  
본질적인 것 이해하기!!(원리 이해) -> 주어진 환경에서 무엇을 바꿔야하는지 이해  

#### 기울기 소실(Gradient Vanishing)  
- 시그모이드 함수 같은 활성화 함수 사용  
- 활성화 함수 특징: 값을 일정 범위 내에 잡아둔다.  
- 기울기 소실 방지: weight vector   
       보통 랜덤으로 돌리거나 1로 함   
       
#### overfiting 방지  
- 다른 데이터 구해서 넣어서 둘 중에 더 나은 거 선택

----
#### 선형 이진 분류  

```
import numpy as np 

def AND(x1, x2):

    x= np.array([x1, x2])

    w = np.array([0.5, 0.5])

    b = 0.0 # b = -0.7로 바꾸면 원하는 결과 나옴

    tmp = np.sum(w*x) + b

    if tmp <= 0:

        return 0

    else:

        return 1
#and gate        
if __name__ == '__main__':

    for xs in [(0, 0), (1, 0), (0, 1), (1, 1)]:

        y = AND(xs[0], xs[1])

        print(str(xs) + " -> " + str(y))
```


> 결과  
> (0, 0) -> 0  
> (1, 0) -> 0  
> (0, 1) -> 0  
> (1, 1) -> 1  
       
  
